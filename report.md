# **$$NLP-Assignment-2$$**
## **$$REPORT$$**
**Name:** Harshit Gupta 

**Roll No:** 2020114017

-----

### ***Explain negative sampling. How do we approximate the word2vec training computation using this technique?***

The network weights are modified during the training of a neural network in order to learn the representations in the training data accurately. When training data is exceedingly huge, it creates a slew of problems in terms of computing expenses. We have utilized **Word2Vec** in our code, which is a neural network-based natural language processing model. Word2vec models run into problems when the size of the training data increases. To address this problem, word2vec models use a technique called **negative sampling**, which allows only a small percentage of network weights to be changed during training.

-----

