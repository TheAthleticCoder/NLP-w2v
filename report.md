# **$$NLP-Assignment-2$$**
## **$$REPORT$$**
**Name:** Harshit Gupta 

**Roll No:** 2020114017

[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://forthebadge.com)

-----

### ***Explain negative sampling. How do we approximate the word2vec training computation using this technique?***

The network weights are modified during the training of a neural network in order to learn the representations in the training data accurately. When training data is exceedingly huge, it creates a slew of problems in terms of computing expenses. We have utilized **Word2Vec** in our code, which is a neural network-based natural language processing model. Word2vec models run into problems when the size of the training data increases. To address this problem, word2vec models use a technique called **negative sampling**, which allows only a small percentage of network weights to be changed during training.

-----
## ***$$Implementation$$***

1. **Model 1:** We implement a word embedding model and train word vectors by first building a Co-occurrence Matrix followed by the application of SVD. 
2. **Model 2:** We implement the word2vec model and train word vectors using the CBOW model with Negative Sampling.

-----

## ***$$Analysis$$***

### **Analysis 1:**
**Requirement:** Display the top-10 word vectors for five different words (a combination of nouns, verbs, adjectives, etc.) using t-SNE (or such methods) on a 2D plot.

The 5 words on which we shall be basing our analysis are:

**The t-SNE graph for `Model 1` is:**

**The t-SNE graph for `Model 2` is:**

### **Analysis 2:**
**Requirement:** What are the top 10 closest words for the word ’camera’ in the embeddings generated by your program? Compare them against the pre-trained word2vec embeddings that you can download off-the-shelf.

**The top 10 closest words for the word ’camera’ using `Model 1`:**

**The top 10 closest words for the word ’camera’ using `Model 2`:**

**The top 10 closest words for the word ’camera’ using the `Pretrained Model`:**

-----

